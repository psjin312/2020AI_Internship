------------------
전이 학습에 대해 알아보자

* 전이 학습이란?
> 전이 학습이란, 영어로 Transfer Learning이고, 특정 분야에서 학습된 신경망의 일부 능력을 유사하거나 전혀 새로운 분야에서 
> 사용되는 신경망의 학습에 이용하는 것을 의미한다.

----------------------

> 데이터마이닝과 기계학습 기술은 분류(classification), 회귀(regression)와 군집(clustering)을 포함한 
> 여러 지식 엔지니어링 분야에서 주목할 만한 성과를 이루었다. 
> 그러나 많은 기계학습 기법들은 훈련 데이터와 실험 데이터가 같은 특징 공간과 같은 분포를 가진다는 것을 
> 가정으로 한 경우에만 효율적이다. 
 
> 따라서 데이터의 분포가 바뀌면 기존의 통계적 모델들을 새로 수집된 훈련 데이터로 다시 만들어야 한다. 
> 현실세계의 응용에서는 훈련 데이터를 새로 수집하거나 모델을 다시 만드는 과정은 비용이 많이 들거나 불가능한 경우가 많다. 
> 따라서 훈련 데이터를 새로 수집하는데 드는 비용을 줄이는 방법이 될 수 있는 지식이전(knowledge transfer) 
> 혹은 전이학습(transfer learning)이 필요하다.

> 전이 학습은 학습 데이터의 수가 적을때도 효과적이며, 학습 속도도 빠르다.   
> 또한, 전이 학습 없이 학습하는 것보다 훨씬 높은 정확도를 제공한다.   
> 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 전이 학습을 사용한다.

> 전이 학습에 대해 쉽게 말하자면, 사과 깎는 방법을 읽힌 AI에게 배를 깎도록 하는 것, 체스를 익힌 AI에게 장기를 두게 하거나,
> 비가 올 확률을 예측하는 AI에게 눈이 올 확률을 예측하게 하는 것이라고 할 수 있다.

* 딥러닝의 뛰어난 성과를 보여주는 '이미지 분류 분야'
> 이미지 분류 문제의 목적은 특정 이미지들을 주어진 카테고리로 정확하게 분류하는 것이다. 
> 이미지 분류에서 전통적으로 유명한 문제로는 '고양이 vs 강아지' 사진 분류 문제가 있다.

> 특히 이미지 분류 문제는 전이 학습으로 접근하는 경우가 대부분이다.

그럼, 이미지 분류 문제에 어떻게 전이 학습을 적용시키는지 알아보자.

![Dog and Cat](https://user-images.githubusercontent.com/34376342/97551661-9b2dba00-1a16-11eb-86ba-832f4996335f.PNG)

> 전이 학습에 이용되는 몇몇의 사전 학습 모델들은 큰 사이즈의 합성곱 신경망 (Convolutional Neural Networks, CNN) 
> 구조를 가지고 있다.
> CNN은 좋은 성능과 동시에 학습을 쉽게 할 수 있다는 두 가지의 메인 요소가 있기 때문에 뜨거운 관심을 받고 있다.

일반적인 CNN은 두 가지 파트로 구성되어 있다.
- Convolutional base
> 합성곱층과 풀링층이 여러겹 쌓여있는 부분    
> Convolutional base의 목표는 이미지로부터 특징을 효과적으로 추출하는 것 (feature extraction)

- Classifier
> 주로 완전 연결 계층(fully connected layer)으로 이루어져 있다.   
> 완전 연결 계층이란 모든 계층의 뉴런이 이전 층의 출력 노드와 하나도 빠짐없이 모두 연결되어 있는 층을 말한다.   
> 분류기(Classifier)의 최종 목표 : 추출된 특징을 잘 학습해서 이미지를 알맞은 카테고리로 분류하는 것(image classification)

> ![CNN process](https://user-images.githubusercontent.com/34376342/97602676-f11e5400-1a4e-11eb-818d-57919a6a6b76.PNG)   
> ![image](https://user-images.githubusercontent.com/34376342/97673199-b956f100-1ace-11eb-9e6c-4648fdadd798.png)

> 모델의 첫 번째 층은 **일반적인(general)** 특징을 추출하도록 하는 학습이 이루어지는 반면에, 모델의 마지막 층에
> 가까워질수록 특정 데이터셋 또는 특정 문제에서만 나타날 수 있는 **구체적인(specific)** 특징을 추출해내도록 하는 고도화된
> 학습이 이루어진다.

- Convolution이란?
> ![image](https://user-images.githubusercontent.com/34376342/97603658-02b42b80-1a50-11eb-92ea-c623c0ca5688.png)

> 데이터의 특징을 추출하는 과정으로 데이터에 각 성분의 인접 성분들을 조사해 특징을 파악하고 파악한 특징을 한장으로 
> 도출시키는 과정이다. 여기서 도출된 장을 Convolution Layer라고 한다. 이 과정은 하나의 압축 과정이며 파라미터의 갯수를 
> 효과적으로 줄여주는 역할을 한다.

- Pooling
> 이는 Convolution 과정을 거친 레이어의 사이즈를 줄여주는 과정이다. 단순히 데이터의 사이즈를 줄여주고, 
> 노이즈를 상쇄시키고 미세한 부분에서 일관적인 특징을 제공한다.

# 사전에 학습된 모델을 내 프로젝트에 맞게 재정의하기
> 사전 학습된 모델을 나의 프로젝트에 맞게 재정의한다면, 원래 모델에 있던 classifier를 없애는 것으로 시작한다.
> 원래의 classifier는 삭제하고, 내 목적에맞는 새로운 classifier를 추가한다. 그 후 마지막으로는 새롭게 만들어진 나의 모델을
> 다음 세 가지 전략 중 한 가지 방법을 이용해 파인튜닝(fine-tune)을 진행한다.

* 1. 전체 모델을 새로 학습시키기
> 방법은 사전학습 모델의 구조만 사용하면서, 내 데이터셋에 맞게 전부 새로 학습시키는 방법이다.   
> 모델을 밑바닥에서부터 새로 습시키는 것이므로 큰 사이즈의 데이터셋이 필요하다. 또한, 좋은 컴퓨팅 연산 능력도 필요하다.

* 2. Convolutional base의 일부분은 고정시킨 상태로, 나머지 계층과 classifier를 새로 학습시키기
> 앞서 말했듯이 낮은 레벨의 계층은 일반적인 특징(어떤 문제를 푸느냐에 상관 없이 독립적인 특징)을 추출하고, 높은 레벨의 계층은
> 구체적이고 특유한 특징(문제에 따라 달라지는 특징)을 추출한다. 이런 특성을 이용해서 우리는 신경망의 파라미터 중 어느 
> 정도까지를 재학습 시킬지를 정할 수 있다.   
> 만약 데이터셋이 작고 모델의 파라미터가 많다면, 과적합(overfitting)이 될 위험이 있으므로 더 많은 계층을 건들지 않고 그대로
> 둔다. 반면에 데이터셋이 크고 그에 비해 모델이 작아서 파라미터가 적다면, 과적합에 대한 걱정을 할 필요가 없으므로 더 많은 
> 계층을 학습시켜서 내 프로젝트에 더 적합한 모델로 발전시킬 수 있다.

* 3. Convolutional base는 고정시키고, classifier만 새로 학습시키기
> 이 경우는 보다 극단적인 상황일 때 생각할 수 있는 케이스이다. convolutional base는 건들지 않고 그대로 두면서 특징 추출 메커니즘으로써 활용하고, classifier만 재학습시키는 방법을 쓴다. 이 방법은 컴퓨팅 연산 능력이 부족하거나 데이터셋이 너무 작을때, 그리고 내가 풀고자 하는 문제의 사전학습모델이 이미 학습한 데이터셋과 매우 비슷할 때 고려해볼 수 있다.
